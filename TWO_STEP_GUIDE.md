# ğŸ—ºï¸ Guide du Scraper en 2 Ã‰tapes

## Architecture Moderne : Entreprises â†’ People

Ce nouveau systÃ¨me sÃ©pare clairement le scraping des **entreprises** et la recherche de **contacts**, avec des colonnes sÃ©parÃ©es par source pour une traÃ§abilitÃ© complÃ¨te.

---

## ğŸ“‹ Workflow

```mermaid
graph LR
    A[Google Maps] --> B[Ã‰tape 1: Entreprises]
    B --> C[Blacklist Filter]
    C --> D[Enrichissement]
    D --> E[Onglet Entreprises]
    E --> F[Ã‰tape 2: People]
    F --> G[Apollo Apify]
    F --> H[Dropcontact]
    F --> I[EmailFinder]
    F --> J[Website Scraping]
    G --> K[Onglet People]
    H --> K
    I --> K
    J --> K
```

---

## 1ï¸âƒ£ Ã‰tape 1 : Scraper les Entreprises

### ğŸ¯ Objectif
RÃ©cupÃ©rer les entreprises depuis Google Maps et les enrichir avec des donnÃ©es officielles.

### ğŸ”§ Fonctionnement
1. **Scraping Google Maps** (via Apify)
2. **Filtrage Blacklist** - Exclut les grandes chaÃ®nes rÃ©currentes
3. **Enrichissement** - SIRET, effectifs, CA (API entreprise.data.gouv.fr)
4. **Export** â†’ Onglet **"Entreprises"** dans Google Sheets

### ğŸ“Š Colonnes de l'onglet "Entreprises"
| Colonne | Description |
|---------|-------------|
| Nom | Nom de l'entreprise |
| Adresse | Adresse complÃ¨te |
| Ville | Ville |
| TÃ©lÃ©phone | NumÃ©ro de tÃ©lÃ©phone |
| Site Web | URL du site |
| Note Google | Note moyenne Google Maps |
| Nombre Avis | Nombre d'avis |
| CatÃ©gorie | CatÃ©gorie d'activitÃ© |
| URL Google Maps | Lien vers la fiche |
| SIRET | NumÃ©ro SIRET |
| SIREN | NumÃ©ro SIREN |
| Forme Juridique | SARL, SAS, etc. |
| Effectifs | Nombre d'employÃ©s |
| CA EstimÃ© | Chiffre d'affaires |
| Date CrÃ©ation | Date de crÃ©ation |
| Date Ajout | Date d'ajout au systÃ¨me |
| Statut | nouveau, en_cours, terminÃ© |
| Nb Contacts TrouvÃ©s | Nombre de contacts trouvÃ©s |

### ğŸ’» Utilisation

**Interface Streamlit** :
```bash
streamlit run app_two_step.py
```

**Code Python** :
```python
from scraper_two_step import TwoStepScraper

scraper = TwoStepScraper()

# Scraper les entreprises
companies = scraper.scrape_companies(
    search_query="fabricants de vÃ©randas Ã  Paris",
    max_results=50,
    location="Ãle-de-France"  # Optionnel
)

print(f"{len(companies)} entreprises scrapÃ©es")
```

---

## 2ï¸âƒ£ Ã‰tape 2 : Chercher les Contacts

### ğŸ¯ Objectif
Pour chaque entreprise de l'onglet "Entreprises", trouver des contacts qualifiÃ©s via **4 sources** diffÃ©rentes.

### ğŸ”§ Fonctionnement
1. **Lecture** - RÃ©cupÃ¨re les entreprises depuis l'onglet
2. **Apollo Apify** (prioritaire) - Contacts vÃ©rifiÃ©s B2B
3. **Dropcontact** (fallback) - Si Apollo ne trouve rien
4. **EmailFinder** - Construction intelligente d'emails
5. **Website Scraping** - Extraction depuis les sites web
6. **Export** â†’ Onglet **"People"** avec **colonnes sÃ©parÃ©es par source**

### ğŸ“Š Colonnes de l'onglet "People"

#### Identification
| Colonne | Description |
|---------|-------------|
| Nom Entreprise | Nom de l'entreprise (FK) |
| Domaine | Domaine du site web |
| Nom Contact | Nom complet du contact |
| Fonction | Poste occupÃ© |
| Localisation | Ville/rÃ©gion |

#### Source 1 : Apollo Apify (ğŸ”µ Prioritaire)
| Colonne | Description |
|---------|-------------|
| Email Apollo | Email trouvÃ© par Apollo |
| Conf. Apollo | Confiance (high/medium/low) |
| Tel Apollo | TÃ©lÃ©phone Apollo |
| LinkedIn Apollo | Profil LinkedIn |

#### Source 2 : Dropcontact (ğŸŸ¢ Fallback)
| Colonne | Description |
|---------|-------------|
| Email Dropcontact | Email trouvÃ© par Dropcontact |
| Conf. Dropcontact | Confiance |
| Tel Dropcontact | TÃ©lÃ©phone Dropcontact |

#### Source 3 : EmailFinder (ğŸŸ¡ Construction)
| Colonne | Description |
|---------|-------------|
| Email Construit | Email construit intelligemment |
| Pattern | Pattern utilisÃ© (prenom.nom@, etc.) |
| Conf. Construit | Confiance du pattern |

#### Source 4 : Website Scraping (ğŸŸ  Extraction)
| Colonne | Description |
|---------|-------------|
| Email ScrapÃ© | Email trouvÃ© sur le site |
| Conf. ScrapÃ© | Confiance |

#### MÃ©tadonnÃ©es
| Colonne | Description |
|---------|-------------|
| Source Principale | apollo, dropcontact, constructed, scraped |
| Email Meilleur | Email avec la meilleure confiance |
| Conf. Meilleure | Confiance du meilleur email |
| Toutes Sources | Liste de toutes les sources utilisÃ©es |
| Date Ajout | Date d'ajout |

### ğŸ’» Utilisation

**Interface Streamlit** :
```bash
streamlit run app_two_step.py
```

**Code Python** :
```python
from scraper_two_step import TwoStepScraper

scraper = TwoStepScraper()

# Chercher les contacts
contacts = scraper.scrape_people(
    job_titles=["CEO", "GÃ©rant", "Directeur Commercial"],
    max_contacts_per_company=3
)

print(f"{len(contacts)} contacts trouvÃ©s")
```

---

## ğŸš« SystÃ¨me de Blacklist

### ğŸ¯ Objectif
Ã‰viter de scraper les mÃªmes grandes chaÃ®nes prÃ©sentes partout (ex: Leroy Merlin, Point.P, etc.)

### ğŸ“ Fichier
`company_blacklist.json`

### ğŸ’» Utilisation

**Interface Streamlit** :
- Sidebar â†’ Section "Blacklist Entreprises"
- Voir/Modifier/Ajouter

**Code Python** :
```python
from company_blacklist import CompanyBlacklist

bl = CompanyBlacklist()

# Ajouter des entreprises
bl.add("Leroy Merlin")
bl.add_multiple(["Akena VÃ©randa", "Vie & VÃ©randa"])

# Filtrer une liste
companies = [...]
filtered = bl.filter_companies(companies, name_key='name')

# VÃ©rifier si blacklistÃ©e
if bl.is_blacklisted("Akena VÃ©randa"):
    print("Cette entreprise est blacklistÃ©e")
```

### ğŸ“ Exemple de Blacklist (VÃ©randas)
```json
{
  "blacklist": [
    "vÃ©randa rideau",
    "akena vÃ©randa",
    "vie & vÃ©randa",
    "gustave rideau",
    "technal",
    "tryba",
    "point.p",
    "leroy merlin"
  ]
}
```

---

## ğŸ”µ Apollo Apify Scraper (Prioritaire)

### ğŸ¯ Pourquoi Apollo via Apify ?
- âœ… **Meilleure qualitÃ©** de donnÃ©es B2B
- âœ… **Emails vÃ©rifiÃ©s** (status: verified/guessed)
- âœ… **Pas de limite** de crÃ©dits Apollo
- âœ… **TÃ©lÃ©phones + LinkedIn** inclus
- âœ… **$0.70 / 1000 contacts** environ

### âš™ï¸ Configuration
1. Obtenir une clÃ© API Apify : https://apify.com
2. Ajouter dans `.env` :
   ```
   APIFY_API_TOKEN=votre_token_apify
   ```

### ğŸ’» Utilisation directe
```python
from apollo_apify_scraper import ApolloApifyScraper

scraper = ApolloApifyScraper()

# Chercher des contacts
contacts = scraper.search_people(
    company_name="Stripe",
    job_titles=["CEO", "CTO", "VP Engineering"],
    max_results=5
)

for contact in contacts:
    print(f"{contact['name']} - {contact['email']}")
```

---

## ğŸ“Š Google Sheets : 2 Onglets

### Onglet "Entreprises"
- 1 ligne = 1 entreprise
- DonnÃ©es Google Maps + enrichissement
- Statut de traitement

### Onglet "People"
- 1 ligne = 1 contact
- **Colonnes sÃ©parÃ©es par source**
- TraÃ§abilitÃ© complÃ¨te

---

## âš¡ Avantages du SystÃ¨me

| Avantage | Description |
|----------|-------------|
| ğŸ¯ **SÃ©paration claire** | Entreprises et contacts sont sÃ©parÃ©s |
| ğŸš« **Blacklist** | Ã‰vite les doublons des grandes chaÃ®nes |
| ğŸ”µ **Apollo prioritaire** | Meilleure qualitÃ© de donnÃ©es |
| ğŸ“Š **Colonnes par source** | TraÃ§abilitÃ© complÃ¨te |
| ğŸ’° **Ã‰conomique** | Ne recherche les contacts qu'une fois |
| ğŸ”„ **RÃ©-exÃ©cutable** | Peut relancer Ã‰tape 2 sans re-scraper Ã‰tape 1 |

---

## ğŸš€ DÃ©marrage Rapide

1. **Installer les dÃ©pendances** :
   ```bash
   pip install apify-client gspread oauth2client python-dotenv
   ```

2. **Configurer `.env`** :
   ```
   APIFY_API_TOKEN=votre_token
   GOOGLE_SHEET_ID=votre_sheet_id
   ```

3. **Lancer l'interface** :
   ```bash
   streamlit run app_two_step.py
   ```

4. **Ã‰tape 1** : Scraper les entreprises
5. **Ã‰tape 2** : Chercher les contacts

---

## ğŸ“ Notes

- **Apollo Apify** est le scraper **prioritaire** pour la qualitÃ©
- Les **colonnes sÃ©parÃ©es** permettent de voir toutes les sources
- La **blacklist** est persistante dans `company_blacklist.json`
- Consultez `scraper_two_step.py` pour le code complet

---

## ğŸ†˜ Support

Pour toute question sur ce systÃ¨me :
1. Consultez les logs dans l'interface Streamlit
2. VÃ©rifiez `company_blacklist.json` pour la blacklist
3. Testez avec `python scraper_two_step.py`
